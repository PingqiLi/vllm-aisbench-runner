# Qwen3-30B-A3B - W4A4 Quantization
# 4-bit quantized model for Ascend NPUs

model:
  name: "Qwen3-30B-A3B"
  precision: "w4a4"
  description: "Qwen3 30B MoE with W4A4 quantization (Ascend optimized)"

vllm:
  model_path: "Qwen/Qwen3-30B-A3B-W4A4"
  host: "localhost"
  port: 8000
  tensor_parallel_size: 1       # Only needs 1 NPU (~15GB)
  quantization: "ascend"
  dtype: "auto"
  gpu_memory_utilization: 0.90
  trust_remote_code: true
  max_num_seqs: 256
  enable_prefix_caching: true
  max_model_len: 32768
  timeout: 600
  log_file: "logs/qwen3_30b_w4a4_vllm.log"

aisbench:
  model: "vllm_api_general_chat"
  mode: "all"
  summarizer: "example"
  merge_ds: true
  max_num_workers: 8
  dump_eval_details: true

# Hardware requirements
hardware:
  min_gpus: 1
  memory_per_gpu: "20GB"
  recommended_gpus: 1
  note: "Requires vllm-ascend for Ascend NPUs"
