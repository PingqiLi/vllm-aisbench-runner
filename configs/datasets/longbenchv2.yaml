# LongBenchV2 - Long Context Understanding
# Evaluation for long-context capabilities (8k-2M words)
# Without CoT (non-thinking mode) - aligned with official Qwen3 evaluation

dataset:
  name: "longbenchv2_gen_0_shot_chat_prompt"
  description: "Long context understanding (without CoT)"

# vLLM configuration override for 128k context
# This will restart vLLM with these parameters when evaluating this dataset
vllm_config_override:
  max_model_len: 131072  # 128k context
  rope_scaling: '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'

model_config:
  max_out_len: 256  # Aligned with official implementation (128-256)
  generation_kwargs:
    temperature: 0.0
    seed: 42

# Note: Requires huggingface-cli for dataset download
# pip install huggingface_hub
# Dataset path: ais_bench/datasets/LongBench-v2/data.json

# Official Qwen3 Evaluation Settings:
# - Context length: 128k (using YaRN scaling factor 4)
# - Mode: without CoT (non-thinking mode)
# - Use with qwen3-30b-a3b-bf16-128k.yaml model config

# WARNING: LongBench-v2 contains very long contexts (8k-2M words)
# Ensure your vLLM max_model_len is sufficient (recommend 131072+)
# Otherwise inputs will exceed context limit and predictions will be empty
