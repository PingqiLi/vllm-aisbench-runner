# Task: Qwen3-30B-A3B (W8A8) on GPQA
# Graduate-level science Q&A with FP8 quantization

task:
  name: "qwen3-30b-w8a8-gpqa"
  model: "Qwen3-30B-A3B"
  precision: "w8a8"
  dataset: "gpqa"

vllm:
  model_path: "Qwen/Qwen3-30B-A3B"
  host: "localhost"
  port: 8000
  tensor_parallel_size: 8
  quantization: "ascend"
  max_model_len: 32768
  timeout: 600
  enforce_eager: true

aisbench:
  dataset: "gpqa_diamond_gen_0_shot_cot_chat_prompt"
  model: "vllm_api_general_chat"
  mode: "all"
  max_out_len: 32000
  max_num_workers: 16
  merge_ds: true
  dump_eval_details: true
