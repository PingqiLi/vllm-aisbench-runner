# Task: Qwen3-30B-A3B (BF16) on LongBenchV2
# Long context understanding (128k context with YaRN)

task:
  name: "qwen3-30b-bf16-longbenchv2"
  model: "Qwen3-30B-A3B"
  precision: "bf16"
  dataset: "longbenchv2"

vllm:
  model_path: "Qwen/Qwen3-30B-A3B"
  host: "localhost"
  port: 8000
  tensor_parallel_size: 2
  dtype: "bfloat16"

  # 128k context configuration with YaRN RoPE scaling
  max_model_len: 131072
  rope_scaling: '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'

  gpu_memory_utilization: 0.90
  trust_remote_code: true
  max_num_seqs: 256
  enable_prefix_caching: true
  timeout: 600

aisbench:
  dataset: "longbenchv2_gen_0_shot_chat_prompt"
  model: "vllm_api_general_chat"
  mode: "all"
  max_out_len: 256
  generation_kwargs:
    temperature: 0.0
    seed: 42
  max_num_workers: 8
  dump_eval_details: true
